{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWvj_97FHMRq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5dp5KlUIYdU"
      },
      "outputs": [],
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    Create adversarial example using FGSM.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Original input image (tensor).\n",
        "    - epsilon: Perturbation amount.\n",
        "    - data_grad: Gradient of the loss with respect to the input image.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image.\n",
        "    \"\"\"\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "\n",
        "    # Adding clipping to maintain [0, 1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "\n",
        "    return perturbed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAUq1FGQIhhY"
      },
      "outputs": [],
      "source": [
        "def generate_adversarial_example(model, image, target_label, epsilon):\n",
        "    \"\"\"\n",
        "    Generate an adversarial example for a given model and input image.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model.\n",
        "    - image: Original input image (tensor).\n",
        "    - target_label: True label of the input image.\n",
        "    - epsilon: Perturbation amount.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image.\n",
        "    \"\"\"\n",
        "    # Set requires_grad attribute of tensor. Important for Attack\n",
        "    image.requires_grad = True\n",
        "\n",
        "    # Forward pass the data through the model\n",
        "    output = model(image)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = F.nll_loss(output, target_label)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Calculate gradients of model in backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect data gradient\n",
        "    data_grad = image.grad.data\n",
        "\n",
        "    # Create adversarial example\n",
        "    perturbed_image = fgsm_attack(image, epsilon, data_grad)\n",
        "\n",
        "    return perturbed_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(tensor_image, filename):\n",
        "    \"\"\"\n",
        "    Save a tensor as an image file.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor_image: The tensor representation of the image (C x H x W).\n",
        "    - filename: The path where the image will be saved.\n",
        "    \"\"\"\n",
        "    # Convert tensor to PIL Image\n",
        "    pil_image = transforms.ToPILImage()(tensor_image.squeeze(0))  # Remove batch dimension\n",
        "    pil_image.save(filename)"
      ],
      "metadata": {
        "id": "1gX5ABB4iYGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OeWg_BLIjBM",
        "outputId": "8dbab925-0c7d-4cc9-d9fc-5df4b6b7af73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 88.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load a pre-trained ResNet18 model\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess your image\n",
        "    img_path = '/content/saqib.jpeg'  # Replace with your actual image path\n",
        "    original_image = Image.open(img_path)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match model input size\n",
        "        transforms.ToTensor(),           # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per model requirements\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Define target label and epsilon value for perturbation\n",
        "    target_label = torch.tensor([1])  # Replace with actual label index (e.g., class index)\n",
        "\n",
        "    epsilon = 0.01  # Adjust this value for desired perturbation level\n",
        "\n",
        "    # Generate adversarial example\n",
        "    adv_image = generate_adversarial_example(model, image_tensor, target_label, epsilon)\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_image(adv_image, 'adversarial_example_khizar.png')  # Save as PNG file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code with Guassian Blur added to smoothen the image out**"
      ],
      "metadata": {
        "id": "PPL0yjQp-oDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NGUrMdkI-n09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    Create adversarial example using FGSM with a lower epsilon for subtle changes.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Original input image (tensor).\n",
        "    - epsilon: Perturbation amount.\n",
        "    - data_grad: Gradient of the loss with respect to the input image.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image.\n",
        "    \"\"\"\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "\n",
        "    # Adding clipping to maintain [0, 1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "vLcLiUsM-8p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_adversarial_example(model, image, target_label, epsilon):\n",
        "    \"\"\"\n",
        "    Generate an adversarial example for a given model and input image.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model.\n",
        "    - image: Original input image (tensor).\n",
        "    - target_label: True label of the input image.\n",
        "    - epsilon: Perturbation amount.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image.\n",
        "    \"\"\"\n",
        "    # Set requires_grad attribute of tensor. Important for Attack\n",
        "    image.requires_grad = True\n",
        "\n",
        "    # Forward pass the data through the model\n",
        "    output = model(image)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = F.nll_loss(output, target_label)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Calculate gradients of model in backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect data gradient\n",
        "    data_grad = image.grad.data\n",
        "\n",
        "    # Create adversarial example with reduced epsilon\n",
        "    perturbed_image = fgsm_attack(image, epsilon, data_grad)\n",
        "\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "3xoES9c1_DCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(tensor_image, filename):\n",
        "    \"\"\"\n",
        "    Save a tensor as an image file.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor_image: The tensor representation of the image (C x H x W).\n",
        "    - filename: The path where the image will be saved.\n",
        "    \"\"\"\n",
        "    # Convert tensor to PIL Image\n",
        "    pil_image = transforms.ToPILImage()(tensor_image.squeeze(0))  # Remove batch dimension\n",
        "    pil_image.save(filename)"
      ],
      "metadata": {
        "id": "SuJJRN07_EK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load a pre-trained ResNet18 model\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess your image\n",
        "    img_path = '/content/saqib.jpeg'  # Path to original image\n",
        "    original_image = Image.open(img_path)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match model input size\n",
        "        transforms.ToTensor(),           # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per model requirements\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Define target label and reduced epsilon value for subtle perturbation\n",
        "    target_label = torch.tensor([1])  # Replace with actual label index (e.g., class index)\n",
        "    epsilon = 0.005  # Reduced epsilon for less visible perturbations\n",
        "\n",
        "    # Generate adversarial example\n",
        "    adv_image = generate_adversarial_example(model, image_tensor, target_label, epsilon)\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_image(adv_image, '/content/adversarial_example_saqib_blur.png')  # Save as PNG file"
      ],
      "metadata": {
        "id": "dlMe1hf-_Gd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code with PGD adversarial attack**"
      ],
      "metadata": {
        "id": "TNNCEFA9_wB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "7Yp4UDY2_3TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(image, model, target_label, epsilon, alpha, num_steps):\n",
        "    \"\"\"\n",
        "    Generate an adversarial example using the PGD method.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Original input image (tensor).\n",
        "    - model: The neural network model.\n",
        "    - target_label: True label of the input image.\n",
        "    - epsilon: Maximum perturbation amount.\n",
        "    - alpha: Step size for each iteration.\n",
        "    - num_steps: Number of steps in the PGD attack.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image.\n",
        "    \"\"\"\n",
        "    # Clone the input image to avoid modifying the original\n",
        "    perturbed_image = image.clone().detach()\n",
        "    perturbed_image.requires_grad = True\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        # Forward pass the data through the model\n",
        "        output = model(perturbed_image)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.nll_loss(output, target_label)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Get the sign of the gradients\n",
        "        data_grad = perturbed_image.grad.data\n",
        "\n",
        "        # Update the image by taking a step in the direction of the gradient\n",
        "        perturbed_image = perturbed_image + alpha * data_grad.sign()\n",
        "\n",
        "        # Clamp the perturbation to ensure it stays within the epsilon ball\n",
        "        perturbation = torch.clamp(perturbed_image - image, -epsilon, epsilon)\n",
        "        perturbed_image = torch.clamp(image + perturbation, 0, 1).detach()  # Ensure values are in [0,1]\n",
        "        perturbed_image.requires_grad = True  # Re-enable gradient computation for the next step\n",
        "\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "gAeVMi68_5qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(tensor_image, filename):\n",
        "    \"\"\"\n",
        "    Save a tensor as an image file.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor_image: The tensor representation of the image (C x H x W).\n",
        "    - filename: The path where the image will be saved.\n",
        "    \"\"\"\n",
        "    # Convert tensor to PIL Image\n",
        "    pil_image = transforms.ToPILImage()(tensor_image.squeeze(0))  # Remove batch dimension\n",
        "    pil_image.save(filename)"
      ],
      "metadata": {
        "id": "0lMQi0ca_9aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load a pre-trained ResNet18 model\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img_path = '/content/saqib.jpeg'  # Replace with your actual image path\n",
        "    original_image = Image.open(img_path)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match model input size\n",
        "        transforms.ToTensor(),           # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as per model requirements\n",
        "    ])\n",
        "\n",
        "    image_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Define target label, epsilon, alpha, and num_steps for the PGD attack\n",
        "    target_label = torch.tensor([1])  # Replace with the desired label index (e.g., class index)\n",
        "    epsilon = 0.005                     # Maximum perturbation\n",
        "    alpha = 0.001                      # Step size\n",
        "    num_steps = 40                     # Number of steps in PGD\n",
        "\n",
        "    # Generate adversarial example using PGD\n",
        "    adv_image = pgd_attack(image_tensor, model, target_label, epsilon, alpha, num_steps)\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_image(adv_image, '/content/pgd_adversarial_example_saqib_pgd.png')"
      ],
      "metadata": {
        "id": "rfmdK-d0AAad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PGD Attack along with keeping image size same as original**"
      ],
      "metadata": {
        "id": "6xqUVS2rBsxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "Qu_4G19QBy7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pgd_attack(model, image, target_label, epsilon, alpha, num_iter):\n",
        "    \"\"\"\n",
        "    Perform PGD attack to create adversarial example.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model.\n",
        "    - image: Original input image (tensor).\n",
        "    - target_label: True label of the input image.\n",
        "    - epsilon: Maximum perturbation.\n",
        "    - alpha: Step size for each iteration.\n",
        "    - num_iter: Number of iterations for PGD.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_image: Adversarial image generated using PGD.\n",
        "    \"\"\"\n",
        "    perturbed_image = image.clone()\n",
        "    perturbed_image.requires_grad = True\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        # Forward pass the data through the model\n",
        "        output = model(perturbed_image)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.nll_loss(output, target_label)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update image with gradient step\n",
        "        perturbed_image = perturbed_image + alpha * perturbed_image.grad.sign()\n",
        "\n",
        "        # Clip perturbation to be within epsilon limit and [0,1] range\n",
        "        perturbation = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)\n",
        "        perturbed_image = torch.clamp(image + perturbation, min=0, max=1).detach_()\n",
        "\n",
        "        # Re-enable gradient computation for the next iteration\n",
        "        perturbed_image.requires_grad = True\n",
        "\n",
        "    return perturbed_image\n"
      ],
      "metadata": {
        "id": "6E12TEcZB2qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(tensor_image, filename, mean, std, original_size):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor image, resizes it to the original size, and saves it as a PNG file.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor_image: The tensor representation of the image (C x H x W).\n",
        "    - filename: The path where the image will be saved.\n",
        "    - mean: List of mean values used for normalization, e.g., [0.485, 0.456, 0.406].\n",
        "    - std: List of standard deviation values used for normalization, e.g., [0.229, 0.224, 0.225].\n",
        "    - original_size: Tuple of the original image size (width, height).\n",
        "    \"\"\"\n",
        "    # Denormalize\n",
        "    denormalized_image = tensor_image.clone()\n",
        "    for c in range(denormalized_image.shape[1]):\n",
        "        denormalized_image[0, c] = denormalized_image[0, c] * std[c] + mean[c]\n",
        "\n",
        "    # Clamp to [0,1]\n",
        "    denormalized_image = torch.clamp(denormalized_image, 0, 1)\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    pil_image = transforms.ToPILImage()(denormalized_image.squeeze(0))  # Remove batch dimension\n",
        "\n",
        "    # Resize to original size\n",
        "    pil_image = pil_image.resize(original_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    # Save image\n",
        "    pil_image.save(filename)"
      ],
      "metadata": {
        "id": "NkUbDb5XB6EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a pre-trained ResNet18 model\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Load and preprocess the input image\n",
        "    img_path = '/content/saqib.jpeg'  # Replace with your actual image path\n",
        "    original_image = Image.open(img_path)\n",
        "    original_size = original_image.size  # Save original size (width, height)\n",
        "\n",
        "    # Transform for the image to match model input\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to model's input size\n",
        "        transforms.ToTensor(),          # Convert to tensor\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "    ])\n",
        "    image_tensor = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Parameters for PGD attack\n",
        "    epsilon = 0.01   # Maximum perturbation\n",
        "    alpha = 0.01    # Step size\n",
        "    num_iter = 50   # Number of iterations\n",
        "    target_label = torch.tensor([1])  # Replace with actual target label index\n",
        "\n",
        "    # Generate adversarial example using PGD\n",
        "    adv_image = pgd_attack(model, image_tensor, target_label, epsilon, alpha, num_iter)\n",
        "\n",
        "    # Save the adversarial image with denormalization and resizing to original size\n",
        "    save_image(adv_image, '/content/sameSize_adversarial_example_saqib1.jpeg',\n",
        "               mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], original_size=original_size)"
      ],
      "metadata": {
        "id": "mKk5r5IiB9nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}